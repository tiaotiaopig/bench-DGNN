data: wikipedia
model: tgn

continuous_batch_size: 200 
random_feats: True
num_hist_steps: 1 

learning_rate:
  - 0.0001
  - 0.005
  - 0.001
  - 0.05
  - 0.01

decoder_learning_rate:
  - 0.0001
  - 0.005
  - 0.001
  - 0.05
  - 0.01
decoder_weight_decay:
  - 0.0
  - 0.0001
#  - 0.001
  - 0.01
#  - 0.1
gcn_parameters:
  attention_heads: 2
  dropout: 0.1
  use_memory: True
  layer_1_feats: 172
  layer_2_feats_same_as_l1: True
  num_layers: 2
  cls_feats: 100
